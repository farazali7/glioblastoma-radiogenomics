{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport json\nimport glob\nimport random\nimport collections\n\nimport numpy as np\nimport pandas as pd\nimport pydicom\nfrom pydicom.pixel_data_handlers.util import apply_voi_lut\nimport cv2\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os, warnings\n\nfrom sklearn.model_selection import StratifiedKFold\nimport tensorflow as tf; print(tf.__version__)\nimport tensorflow_addons as tfa\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nwarnings.filterwarnings('ignore')\n# If memory growth is enabled for a PhysicalDevice, the runtime initialization will not allocate all memory on the device.\nphysical_devices = tf.config.list_physical_devices('GPU')\ntry: \n    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n    assert tf.config.experimental.get_memory_growth(physical_devices[0])\nexcept:\n    # Invalid device or cannot modify virtual devices once initialized.\n    pass \n\n# For reproducible results    \ndef seed_all(s):\n    random.seed(s)\n    np.random.seed(s)\n    tf.random.set_seed(s)\n    os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n    os.environ['PYTHONHASHSEED'] = str(s) \n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-01-17T03:13:25.163078Z","iopub.execute_input":"2022-01-17T03:13:25.163610Z","iopub.status.idle":"2022-01-17T03:13:31.488495Z","shell.execute_reply.started":"2022-01-17T03:13:25.163506Z","shell.execute_reply":"2022-01-17T03:13:31.487623Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"train_sample_path = \"../input/rsna-miccai-brain-tumor-radiogenomic-classification/train\"\ntrain_df = pd.read_csv(\"../input/rsna-miccai-brain-tumor-radiogenomic-classification/train_labels.csv\")\ntrain_df","metadata":{"execution":{"iopub.status.busy":"2022-01-17T03:13:52.356936Z","iopub.execute_input":"2022-01-17T03:13:52.357197Z","iopub.status.idle":"2022-01-17T03:13:52.386424Z","shell.execute_reply.started":"2022-01-17T03:13:52.357168Z","shell.execute_reply":"2022-01-17T03:13:52.385699Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"print(\"Test samples: \" + str(len(glob.glob(\"../input/rsna-miccai-brain-tumor-radiogenomic-classification/test/*\"))))","metadata":{"execution":{"iopub.status.busy":"2022-01-17T01:15:42.326745Z","iopub.execute_input":"2022-01-17T01:15:42.327086Z","iopub.status.idle":"2022-01-17T01:15:42.364233Z","shell.execute_reply.started":"2022-01-17T01:15:42.327041Z","shell.execute_reply":"2022-01-17T01:15:42.363223Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(5, 5))\nsns.countplot(data=train_df, x=\"MGMT_value\");","metadata":{"execution":{"iopub.status.busy":"2022-01-17T01:15:42.366819Z","iopub.execute_input":"2022-01-17T01:15:42.367174Z","iopub.status.idle":"2022-01-17T01:15:42.607434Z","shell.execute_reply.started":"2022-01-17T01:15:42.367117Z","shell.execute_reply":"2022-01-17T01:15:42.606431Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_dicom(path):\n    dicom = pydicom.read_file(path)\n    data = dicom.pixel_array\n    data = data - np.min(data)\n    if np.max(data) != 0:\n        data = data / np.max(data)\n    data = (data * 255).astype(np.uint8)\n    return data\n\n\ndef visualize_sample(\n    brats21id, \n    slice_i,\n    mgmt_value,\n    types=(\"FLAIR\", \"T1w\", \"T1wCE\", \"T2w\")\n):\n    plt.figure(figsize=(16, 5))\n    patient_path = os.path.join(\n        \"../input/rsna-miccai-brain-tumor-radiogenomic-classification/train/\", \n        str(brats21id).zfill(5),\n    )\n    for i, t in enumerate(types, 1):\n        t_paths = sorted(\n            glob.glob(os.path.join(patient_path, t, \"*\")), \n            key=lambda x: int(x[:-4].split(\"-\")[-1]),\n        )\n        data = load_dicom(t_paths[int(len(t_paths) * slice_i)])\n        plt.subplot(1, 4, i)\n        plt.imshow(data, cmap=\"gray\")\n        plt.title(f\"{t}\", fontsize=16)\n        plt.axis(\"off\")\n\n    plt.suptitle(f\"MGMT_value: {mgmt_value}\", fontsize=16)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-17T03:13:58.831466Z","iopub.execute_input":"2022-01-17T03:13:58.831720Z","iopub.status.idle":"2022-01-17T03:13:58.840716Z","shell.execute_reply.started":"2022-01-17T03:13:58.831692Z","shell.execute_reply":"2022-01-17T03:13:58.840044Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"for i in random.sample(range(train_df.shape[0]), 3):\n    _brats21id = train_df.iloc[i][\"BraTS21ID\"]\n    _mgmt_value = train_df.iloc[i][\"MGMT_value\"]\n    visualize_sample(brats21id=_brats21id, mgmt_value=_mgmt_value, slice_i=0.5)","metadata":{"execution":{"iopub.status.busy":"2022-01-17T01:15:42.623438Z","iopub.execute_input":"2022-01-17T01:15:42.624044Z","iopub.status.idle":"2022-01-17T01:15:44.643324Z","shell.execute_reply.started":"2022-01-17T01:15:42.623986Z","shell.execute_reply":"2022-01-17T01:15:44.642404Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from matplotlib import animation, rc\nrc('animation', html='jshtml')\n\n\ndef create_animation(ims):\n    fig = plt.figure(figsize=(6, 6))\n    plt.axis('off')\n    im = plt.imshow(ims[0], cmap=\"gray\")\n\n    def animate_func(i):\n        im.set_array(ims[i])\n        return [im]\n\n    return animation.FuncAnimation(fig, animate_func, frames = len(ims), interval = 1000//24)","metadata":{"execution":{"iopub.status.busy":"2022-01-17T01:15:44.645213Z","iopub.execute_input":"2022-01-17T01:15:44.645669Z","iopub.status.idle":"2022-01-17T01:15:44.657306Z","shell.execute_reply.started":"2022-01-17T01:15:44.645501Z","shell.execute_reply":"2022-01-17T01:15:44.656245Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_dicom_line(path):\n    t_paths = sorted(\n        glob.glob(os.path.join(path, \"*\")), \n        key=lambda x: int(x[:-4].split(\"-\")[-1]),\n    )\n    images = []\n    for filename in t_paths:\n        data = load_dicom(filename)\n        if data.max() == 0:\n            continue\n        images.append(data)\n        \n    return images","metadata":{"execution":{"iopub.status.busy":"2022-01-17T03:14:04.710876Z","iopub.execute_input":"2022-01-17T03:14:04.711136Z","iopub.status.idle":"2022-01-17T03:14:04.716229Z","shell.execute_reply.started":"2022-01-17T03:14:04.711110Z","shell.execute_reply":"2022-01-17T03:14:04.715573Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"images = load_dicom_line(\"../input/rsna-miccai-brain-tumor-radiogenomic-classification/train/00077/FLAIR\")\ncreate_animation(images)","metadata":{"execution":{"iopub.status.busy":"2022-01-17T01:15:44.669665Z","iopub.execute_input":"2022-01-17T01:15:44.670096Z","iopub.status.idle":"2022-01-17T01:15:50.457354Z","shell.execute_reply.started":"2022-01-17T01:15:44.670039Z","shell.execute_reply":"2022-01-17T01:15:50.456533Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size    = 1   # Number of the batch size\naccum_step    = 3   # Gradient accumulation steps\ninput_height  = 120\ninput_width   = 120\ninput_channel = 4  # Total number of channel, e.g. 4 \ninput_depth   = 30 # Total number of slices from each modality, e.g. 30 \ntotal_fold = 5\nfold = 0\nglobal_seed = 7\nseed_all(global_seed)\n\nmodalities = [\"FLAIR\", \"T1w\", \"T1wCE\", \"T2w\"] \n\nAUTO = tf.data.AUTOTUNE","metadata":{"execution":{"iopub.status.busy":"2022-01-17T03:14:07.610314Z","iopub.execute_input":"2022-01-17T03:14:07.610746Z","iopub.status.idle":"2022-01-17T03:14:07.617892Z","shell.execute_reply.started":"2022-01-17T03:14:07.610710Z","shell.execute_reply":"2022-01-17T03:14:07.615885Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"skf = StratifiedKFold(n_splits=total_fold, shuffle=True, random_state=global_seed)\n\nfor index, (train_index, val_index) in enumerate(skf.split(X=train_df.index, \n                                                           y=train_df.MGMT_value)):\n    train_df.loc[val_index, 'fold'] = index\n    \nprint('Ground Truth Distribution Fold-Wise..')\nprint(train_df.groupby(['fold', train_df.MGMT_value]).size())","metadata":{"execution":{"iopub.status.busy":"2022-01-17T03:14:13.675383Z","iopub.execute_input":"2022-01-17T03:14:13.676029Z","iopub.status.idle":"2022-01-17T03:14:13.708479Z","shell.execute_reply.started":"2022-01-17T03:14:13.675984Z","shell.execute_reply":"2022-01-17T03:14:13.707799Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"train_df","metadata":{"execution":{"iopub.status.busy":"2022-01-17T01:15:50.502736Z","iopub.execute_input":"2022-01-17T01:15:50.503615Z","iopub.status.idle":"2022-01-17T01:15:50.523232Z","shell.execute_reply.started":"2022-01-17T01:15:50.503568Z","shell.execute_reply":"2022-01-17T01:15:50.521947Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Generate brain tumour image data from path\nclass DataGenerator(keras.utils.Sequence):\n    def __init__(self, dicom_path, data, is_train=True):\n        self.is_train = is_train # to control training/validation/inference part \n        self.data = data\n        self.dicom_path = dicom_path\n        self.label = self.data['MGMT_value']\n  \n    def __len__(self):\n        return len(self.data['BraTS21ID'])\n    \n    def __getitem__(self, index):\n        patient_ids = f\"{self.dicom_path}/{str(self.data['BraTS21ID'][index]).zfill(5)}/\"\n    \n        flair = []\n        t1w   = []\n        t1wce = []\n        t2w   = [] \n        \n        # Iterating over each modality\n        for m, t in enumerate(modalities):\n            t_paths = sorted(\n                glob.glob(os.path.join(patient_ids, t, \"*\")), \n                key=lambda x: int(x[:-4].split(\"-\")[-1]),\n            )\n            \n            # Pick input_depth times slices -\n            # - from middle range possible \n            strt_idx = (len(t_paths) // 2) - (input_depth // 2)\n            end_idx = (len(t_paths) // 2) + (input_depth // 2)\n            # slicing extracting elements with 1 intervals \n            picked_slices = t_paths[strt_idx:end_idx:1]\n            \n            # Preprocess picked slices (remove black border + bind together)\n            for i in picked_slices:\n                # Reading pixel file from dicom file \n                image = self.read_dicom_xray(i)\n                \n                # Iterate and randomly replace an image that is fully black\n                j = 0\n                while True:\n                    # if it's a black image, try to pick any random slice of non-black  \n                    # otherwise move on with black image. \n                    if image.mean() == 0:\n                        image = self.read_dicom_xray(random.choice(t_paths)) \n                        j += 1\n                        if j == 100:\n                            break\n                    else:\n                        break\n                        \n                # Now, we remove black areas; remove black borders from brain image \n                rows = np.where(np.max(image, 0) > 0)[0]\n                cols = np.where(np.max(image, 1) > 0)[0]\n                if rows.size:\n                    image = image[cols[0]: cols[-1] + 1, rows[0]: rows[-1] + 1]\n                else:\n                    image = image[:1, :1]\n           \n                # Add frames / slices of individual modalities \n                if m == 0:\n                    # Adding flair \n                    flair.append(cv2.resize(image, (input_height, input_width)))\n                elif m == 1:\n                    # Adding t1w\n                    t1w.append(cv2.resize(image, (input_height, input_width)))\n                elif m == 2:\n                    # Adding t1wce\n                    t1wce.append(cv2.resize(image, (input_height, input_width)))\n                elif m == 3:\n                    # Adding t2w\n                    t2w.append(cv2.resize(image, (input_height, input_width)))\n                    \n        # input_shape: (None, h, w, depth, channel)\n        # Resample modality arrays that have less than input depth no. of slices\n\n        # for flair \n        while True:\n            if len(flair) < input_depth and flair:\n                flair.append(cv2.convertScaleAbs(random.choice(flair), alpha=1.2, beta=0))\n            else:\n                break\n\n        # for t1w\n        while True:\n            if len(t1w) < input_depth and t1w:\n                t1w.append(cv2.convertScaleAbs(random.choice(t1w), alpha=1.1, beta=0))\n            else:\n                break\n\n        # for t1wce\n        while True:\n            if len(t1wce) < input_depth and t1wce:\n                t1wce.append(cv2.convertScaleAbs(random.choice(t1wce), alpha=1.2, beta=0))\n            else:\n                break\n\n        # for t2w\n        while True:\n            if len(t2w) < input_depth and t2w:\n                t2w.append(cv2.convertScaleAbs(random.choice(t2w), alpha=1.1, beta=0))\n            else:\n                break\n\n        return np.array((flair, t1w, t1wce, t2w),\n                        dtype=\"object\").T, self.label.iloc[index,]\n            \n        \n    # Function to read dicom file \n    def read_dicom_xray(self, path):\n        data = pydicom.read_file(path).pixel_array\n        if data.mean() == 0:\n            # If all black, return data and find non-black if possible.\n            return data \n        data = data - np.min(data)\n        data = data / np.max(data)\n        data = (data * 255).astype(np.uint8)\n        return data","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-17T03:14:20.329889Z","iopub.execute_input":"2022-01-17T03:14:20.330321Z","iopub.status.idle":"2022-01-17T03:14:20.353403Z","shell.execute_reply.started":"2022-01-17T03:14:20.330285Z","shell.execute_reply":"2022-01-17T03:14:20.352685Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"def fold_generator(fold):\n    train_labels = train_df[train_df.fold != fold].reset_index(drop=True)\n    print(train_labels)\n    val_labels   = train_df[train_df.fold == fold].reset_index(drop=True)\n    return (\n        DataGenerator(train_sample_path, train_labels),\n        DataGenerator(train_sample_path, val_labels)\n    )\n\n# Get fold set\ntrain_gen, val_gen = fold_generator(fold)","metadata":{"execution":{"iopub.status.busy":"2022-01-17T03:14:23.960577Z","iopub.execute_input":"2022-01-17T03:14:23.960844Z","iopub.status.idle":"2022-01-17T03:14:23.973918Z","shell.execute_reply.started":"2022-01-17T03:14:23.960816Z","shell.execute_reply":"2022-01-17T03:14:23.973221Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"train_data = tf.data.Dataset.from_generator(\n    lambda: map(tuple, train_gen),\n    (tf.float32, tf.float32),\n    (\n        tf.TensorShape([input_height, input_width, input_depth, input_channel]),\n        tf.TensorShape([]),\n    ),\n)\n\nval_data = tf.data.Dataset.from_generator(\n    lambda: map(tuple, val_gen),\n    (tf.float32, tf.float32),\n    (\n        tf.TensorShape([input_height, input_width, input_depth, input_channel]),\n        tf.TensorShape([]),\n    ),\n)","metadata":{"execution":{"iopub.status.busy":"2022-01-17T03:14:26.763370Z","iopub.execute_input":"2022-01-17T03:14:26.763632Z","iopub.status.idle":"2022-01-17T03:14:28.708599Z","shell.execute_reply.started":"2022-01-17T03:14:26.763602Z","shell.execute_reply":"2022-01-17T03:14:28.707918Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"def tf_image_augmentation(image):  \n    splitted_modalities = tf.split(tf.cast(image, tf.float32), input_channel, axis=-1) \n \n    flair_augment_img = []\n    t1w_augment_img   = []\n    t1wce_augment_img = []\n    t2w_augment_img   = []\n    \n    # Remove last axis so we go from (h, w, input_depth, 1) to (h, w, input_depth)\n    splitted_modalities = [tf.squeeze(i, axis=-1) for i in splitted_modalities] \n    \n    # iterate over each modality, e.g: flair, t1w, t1wce, t2w\n    for j, modality in enumerate(splitted_modalities):\n        # now splitting each frame from one modality \n        splitted_frames = tf.split(tf.cast(modality, tf.float32), modality.shape[-1], axis=-1)\n        \n        # iterate over each frame to conduct same augmentation on each frame \n        for i, img in enumerate(splitted_frames):\n            # Get deterministic augmentation results of each modality. \n            tf.random.set_seed(j)\n            np.random.seed(j)\n            \n            # In tf.image.stateless_random_* , the seed is a Tensor of shape (2,) whose values are any integers.\n            img = tf.image.stateless_random_flip_left_right(img, seed = (j, 2))\n            img = tf.image.stateless_random_flip_up_down(img, seed = (j, 2))\n            img = tf.image.stateless_random_contrast(img, 0.4, 0.8, seed = (j, 2))\n            img = tf.image.stateless_random_brightness(img, 0.3, seed = (j, 2))\n            \n            # Some operations require channel == 3 \n            img = tf.image.stateless_random_saturation(tf.image.grayscale_to_rgb(img), \n                                                       0.9, 1.8, seed = (j, 2))\n            img = tf.image.stateless_random_hue(img, 0.4, seed = (j, 2))\n\n            # Some operations we don't need channel == 3, just 1 is enough \n            img = tf.image.rgb_to_grayscale(img)\n            img = tf.cast(\n                tf.image.stateless_random_jpeg_quality(\n                    tf.cast(img, tf.uint8), \n                    min_jpeg_quality=20, max_jpeg_quality=40, seed = (j, 2)\n                ), tf.float32)\n\n            # Ensuring same augmentation for each modalities \n            if tf.random.uniform((), seed=j) > 0.7:\n                kimg = np.random.choice([1,2,3,4])\n                kgma = np.random.choice([0.7, 0.9, 1.2])\n                \n                img = tf.image.rot90(img, k=kimg) # Random rotation of any 90, 180, 270, 360 \n                img = tf.image.adjust_gamma(img, gamma=kgma) # Adjust the gamma \n                noise = tf.random.normal(shape=tf.shape(img), mean=0.0, stddev=0.2,\n                                         dtype=tf.float32, seed=j) \n                img = img + noise # additive gaussian noise to image \n\n            # The mask_size should be divisible by 2. \n            if tf.random.uniform((), seed=j) > 0.6:\n                img = tfa.image.random_cutout(tf.expand_dims(img, 0),\n                                              mask_size=(int(input_height * 0.2),\n                                                         int(input_width * 0.2)), \n                                              constant_values=0) \n                img = tf.squeeze(img, axis=0)\n            \n            # Clipping. We'll rescale later. \n            img = tf.clip_by_value(img, 0, 255)\n            \n            # Gathering all frames \n            if j == 0: # 1st modality \n                flair_augment_img.append(img)\n            elif j == 1: # 2nd modality \n                t1w_augment_img.append(img)\n            elif j == 2: # 3rd modality \n                t1wce_augment_img.append(img)\n            elif j == 3:  # 4th modality \n                t2w_augment_img.append(img)\n    \n    image = tf.transpose([flair_augment_img, t1w_augment_img, \n                          t1wce_augment_img, t2w_augment_img])\n    image = tf.reshape(image, [input_height, input_width, input_depth, input_channel])\n    return image","metadata":{"execution":{"iopub.status.busy":"2022-01-17T01:15:52.989627Z","iopub.execute_input":"2022-01-17T01:15:52.989953Z","iopub.status.idle":"2022-01-17T01:15:53.013981Z","shell.execute_reply.started":"2022-01-17T01:15:52.989907Z","shell.execute_reply":"2022-01-17T01:15:53.012647Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TFDataGenerator:\n    def __init__(self, \n                 data, \n                 shuffle, \n                 aug_lib,\n                 batch_size, \n                 rescale):\n        self.data        = data             # data files \n        self.shuffle     = shuffle          # true for training \n        self.aug_lib     = aug_lib          # type of augmentation library \n        self.batch_size  = batch_size       # batch size number \n        self.rescale     = rescale          # normalize or not \n            \n    def get_3D_data(self):\n        # augmentation on 3D data set\n        if self.aug_lib == 'tf' and self.shuffle:\n            self.data = self.data.map(lambda x, y: (tf_image_augmentation(x), y),num_parallel_calls=AUTO)\n            self.data = self.data.batch(self.batch_size,drop_remainder=self.shuffle)\n        else:\n            # true for evaluation and inference, no augmentation \n            self.data = self.data.batch(self.batch_size, drop_remainder=self.shuffle)\n        \n        # rescaling the data for faster convergence \n        if self.rescale:    \n            self.data = self.data.map(lambda x, y: (layers.Rescaling(scale=1./255, offset=0.0)(x), y), \n                                      num_parallel_calls=AUTO)\n            \n        # prefetching the data \n        return self.data.prefetch(-1) ","metadata":{"execution":{"iopub.status.busy":"2022-01-17T03:14:36.200559Z","iopub.execute_input":"2022-01-17T03:14:36.200834Z","iopub.status.idle":"2022-01-17T03:14:36.209060Z","shell.execute_reply.started":"2022-01-17T03:14:36.200805Z","shell.execute_reply":"2022-01-17T03:14:36.208119Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"tf_gen = TFDataGenerator(\n    train_data,\n    shuffle     = True,     \n    aug_lib     = 'tf',  \n    batch_size  = batch_size,   \n    rescale     = True\n)  \n\ntrain_generator = tf_gen.get_3D_data()","metadata":{"execution":{"iopub.status.busy":"2022-01-17T03:14:39.206061Z","iopub.execute_input":"2022-01-17T03:14:39.206305Z","iopub.status.idle":"2022-01-17T03:14:39.553673Z","shell.execute_reply.started":"2022-01-17T03:14:39.206278Z","shell.execute_reply":"2022-01-17T03:14:39.552284Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"tf_gen = TFDataGenerator(\n    val_data,\n    shuffle     = False,     \n    aug_lib     = None,  \n    batch_size  = batch_size,   \n    rescale     = True\n)  \n\nvalid_generator = tf_gen.get_3D_data()","metadata":{"execution":{"iopub.status.busy":"2022-01-17T03:14:51.079596Z","iopub.execute_input":"2022-01-17T03:14:51.080189Z","iopub.status.idle":"2022-01-17T03:14:51.134254Z","shell.execute_reply.started":"2022-01-17T03:14:51.080152Z","shell.execute_reply":"2022-01-17T03:14:51.133535Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"class BrainTumorModel3D(keras.Model):\n    def __init__(self, \n                 model,           # Subclass Model \n                 n_gradients=1,   # e.g total_batch_size = batch_size * n_gradients\n                 *args, **kwargs):\n        super(BrainTumorModel3D, self).__init__(*args, **kwargs)\n        self.model = model\n        self.n_gradients = tf.constant(n_gradients, dtype=tf.int32)\n        self.n_acum_step = tf.Variable(0, dtype=tf.int32, trainable=False)\n        self.gradient_accumulation = [tf.Variable(tf.zeros_like(v, dtype=tf.float32), \n                                                  trainable=False) \n                                      for v in self.model.trainable_variables]\n\n    # The training step, forward and backward propagation \n    def train_step(self, data):\n        # Adding 1 to num_acum_step till n_gradients and start GA\n        self.n_acum_step.assign_add(1)\n        # Unpack the data \n        images, labels = data\n\n        # Open a GradientTape\n        with tf.GradientTape() as tape:\n            # Run the forward pass of the layer or model.\n            # Record operations done by layer onto input on the gradient tape.\n            predictions = self.model(images, training=True)\n            # Compute the loss value for this minibatch.\n            loss = self.compiled_loss(labels, predictions)\n        \n        # Compute batch gradients\n        gradients = tape.gradient(loss, self.model.trainable_variables)\n        \n        # Accumulating the batch gradients\n        for i in range(len(self.gradient_accumulation)):\n            self.gradient_accumulation[i].assign_add(gradients[i])\n \n        # If n_acum_step reach the n_gradients then we apply accumulated gradients to update the variables otherwise do nothing\n        tf.cond(tf.equal(self.n_acum_step, self.n_gradients),\n                self.apply_accu_gradients, lambda: None)\n\n        # update metrics\n        self.compiled_metrics.update_state(labels, predictions)\n        return {m.name: m.result() for m in self.metrics}\n    \n    # Function for applying Gradient Accum. \n    def apply_accu_gradients(self):\n        # Apply accumulated gradients\n        self.optimizer.apply_gradients(zip(self.gradient_accumulation, \n                                           self.model.trainable_variables))\n\n        # Reset\n        self.n_acum_step.assign(0)\n        for i in range(len(self.gradient_accumulation)):\n            self.gradient_accumulation[i].assign(\n                tf.zeros_like(self.model.trainable_variables[i],  dtype=tf.float32)\n            )\n\n    # The test step for evaluation and inference \n    def test_step(self, data):\n        # Unpack the data \n        images, labels = data\n        \n        # Run model on inference mode \n        predictions = self.model(images, training=False)\n        \n        # Compute the loss value for this minibatch.\n        loss = self.compiled_loss(labels, predictions)\n        \n        # Update metrics\n        self.compiled_metrics.update_state(labels,  predictions)\n        return {m.name: m.result() for m in self.metrics}\n    \n    # A call funciton needs to be implemented \n    def call(self, inputs, *args, **kwargs):\n        return self.model(inputs)\n    \n    # A custom l2 regularization loss for model to tackle overfit \n    def reg_l2_loss(self, weight_decay = 1e-5):\n        return weight_decay * tf.add_n([\n            tf.nn.l2_loss(v)\n            for v in self.model.trainable_variables\n        ])","metadata":{"execution":{"iopub.status.busy":"2022-01-17T03:14:54.229851Z","iopub.execute_input":"2022-01-17T03:14:54.230204Z","iopub.status.idle":"2022-01-17T03:14:54.246755Z","shell.execute_reply.started":"2022-01-17T03:14:54.230171Z","shell.execute_reply":"2022-01-17T03:14:54.245980Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"pip install classification-models-3D","metadata":{"execution":{"iopub.status.busy":"2022-01-17T03:14:58.300273Z","iopub.execute_input":"2022-01-17T03:14:58.300535Z","iopub.status.idle":"2022-01-17T03:15:08.512344Z","shell.execute_reply.started":"2022-01-17T03:14:58.300506Z","shell.execute_reply":"2022-01-17T03:15:08.511488Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"pip install keras_applications","metadata":{"execution":{"iopub.status.busy":"2022-01-17T03:15:11.141562Z","iopub.execute_input":"2022-01-17T03:15:11.142084Z","iopub.status.idle":"2022-01-17T03:15:20.178338Z","shell.execute_reply.started":"2022-01-17T03:15:11.142035Z","shell.execute_reply":"2022-01-17T03:15:20.177468Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"from classification_models_3D.tfkeras import Classifiers  \n\n# build models \ninput_tensor = keras.Input((input_height, input_width, \n                            input_depth, input_channel), name='input3D')\nmapping3feat = keras.layers.Conv3D(3, (3,3,3), \n                                   strides=(1, 1, 1), \n                                   padding='same', \n                                   use_bias=True)(input_tensor)\n\nresnet50, _ = Classifiers.get('resnet50')\nfeat_ext = resnet50(input_shape=(input_height, input_width,input_depth, 3), \n                       include_top=False, weights='imagenet')\n\noutput = feat_ext(mapping3feat)\noutput = keras.layers.GlobalAveragePooling3D(keepdims=False)(output)\noutput = keras.layers.Dense(1, activation='sigmoid')(output)\nmodel  = keras.Model(input_tensor, output)\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2022-01-17T03:15:22.930331Z","iopub.execute_input":"2022-01-17T03:15:22.930696Z","iopub.status.idle":"2022-01-17T03:15:36.667528Z","shell.execute_reply.started":"2022-01-17T03:15:22.930652Z","shell.execute_reply":"2022-01-17T03:15:36.666822Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"keras.backend.clear_session()\nmodel3D = BrainTumorModel3D(model, n_gradients = batch_size*accum_step)\n\n# compiling \nmodel3D.compile(\n    loss=tfa.losses.SigmoidFocalCrossEntropy(reduction=tf.keras.losses.Reduction.SUM),\n    optimizer=keras.optimizers.Adam(),\n    metrics=[keras.metrics.AUC(), keras.metrics.BinaryAccuracy(name='acc')],\n)","metadata":{"execution":{"iopub.status.busy":"2022-01-17T01:17:37.738538Z","iopub.execute_input":"2022-01-17T01:17:37.738914Z","iopub.status.idle":"2022-01-17T01:17:38.691972Z","shell.execute_reply.started":"2022-01-17T01:17:37.738869Z","shell.execute_reply":"2022-01-17T01:17:38.690261Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CustomModelCheckpoint(keras.callbacks.Callback):\n    def on_train_begin(self, logs=None):\n        self.val_loss = []\n        self.val_auc  = []\n        self.val_acc  = []\n    def on_epoch_end(self, epoch, logs=None):\n        current_val_loss = logs.get(\"val_loss\")\n        current_val_auc  = logs.get('val_auc')\n        current_val_acc  = logs.get('val_acc')\n        self.val_loss.append(current_val_loss)\n        self.val_auc.append(current_val_auc)\n        self.val_acc.append(current_val_acc)\n        \n        # save based on lowest validation loss \n        if current_val_loss <= min(self.val_loss):\n            print('Found lowest val_loss. Saving model weight.')\n            self.model.save_weights('model_at_val_loss.h5') \n            \n        # save based on highest validation auc \n        if current_val_auc >= max(self.val_auc):\n            print('Found highest val_auc. Saving model weight.')\n            self.model.save_weights('model_at_val_auc.h5') \n        \n        # save based on highest validation acc\n        if current_val_acc >= max(self.val_acc):\n            print('Found highest val_acc. Saving model weight.')\n            self.model.save_weights('model_at_val_acc.h5') ","metadata":{"execution":{"iopub.status.busy":"2022-01-17T01:17:38.69365Z","iopub.execute_input":"2022-01-17T01:17:38.694373Z","iopub.status.idle":"2022-01-17T01:17:38.717326Z","shell.execute_reply.started":"2022-01-17T01:17:38.694322Z","shell.execute_reply":"2022-01-17T01:17:38.716542Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# epoch params\nepochs = 5\n\nmodel3D.fit(\n    train_generator,\n    epochs=epochs,\n    validation_data=valid_generator,\n    callbacks=[CustomModelCheckpoint(), \n               tf.keras.callbacks.CSVLogger('history.csv')], \n    verbose=1\n)","metadata":{"execution":{"iopub.status.busy":"2022-01-17T01:22:44.454695Z","iopub.execute_input":"2022-01-17T01:22:44.455239Z"},"trusted":true},"execution_count":null,"outputs":[]}]}